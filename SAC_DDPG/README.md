In **SAC_discrete.ipynb** notebook, we implement and experiment with the Soft Actor-Critic (SAC) algorithm for discrete action spaces using PyTorch on the classic CartPole environment.
It includes environment setup, neural network definitions, training loops, replay buffer management, and evaluation routines, making it a complete, reproducible reference for discrete SAC in reinforcement learning tasks. 

<p align="center">
<img src="https://github.com/ahmad-karami/Reinforcement-Learning/blob/main/SAC_DDPG/assets/CartPole.gif" alt="CartPole SAC vs DDPG" width="600">
</p>


In **SAC_DDPG_continuous.ipynb** notebook, we implement and compare Deep Deterministic Policy Gradient (DDPG) and Soft Actor-Critic (SAC) algorithms for continuous action spaces using PyTorch in the HalfCheetah MuJoCo environment.

<p align="center">
<img src="https://github.com/ahmad-karami/Reinforcement-Learning/blob/main/SAC_DDPG/assets/DDPG.png" alt="DDPG Architecture" width="600">
</p>

<p align="center">
<img src="https://github.com/ahmad-karami/Reinforcement-Learning/blob/main/SAC_DDPG/assets/SAC.png" alt="SAC Architecture" width="600">
</p>



